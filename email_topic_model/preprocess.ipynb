{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Name: Preprocess**\n",
    "\n",
    "**Overview:** Step 2: Performs NLP transformations on all of the message body text for the specific process.\n",
    "                \n",
    "**Data Scientist:** Aaron Medina\n",
    "\n",
    "**GitHub:** https://github.com/aaronjmedina/email-topic-model.git\n",
    "\n",
    "**Creation Date:** 10/27/2022\n",
    "\n",
    "**Instance:** Local\n",
    "\n",
    "**References:** \n",
    "\n",
    "**Script Change Notes:**\n",
    "\n",
    "x/x/xxxx: Aaron - Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import re\n",
    "import pickle\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aaron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aaron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aaron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\aaron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import nltk NLP packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 3,
>>>>>>> 11d905ad626cd2c5b5a4517424a25f8e6b91cb6e
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "pdf_output_path = 'data/pdf.pkl'\n",
    "train_pdf_path = 'data/train_pdf.pkl'\n",
    "test_pdf_path = 'data/test_pdf.pkl'\n",
    "train_corpus_path = 'data/train_corpus.pkl'\n",
    "test_corpus_path = 'data/test_corpus.pkl'\n",
    "tf_vectorizer_model_path = 'models/tf_vectorizer_model.pkl'\n",
    "tfidf_vectorizer_model_path = 'models/tfidf_vectorizer_model.pkl'\n",
    "train_dtm_tfidf_path = 'data/train_dtm_tfidf.pkl'\n",
    "\n",
    "train_test_split_perc = 0.80\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load etl data\n",
    "pdf = pickle.load(open(pdf_output_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text preparation removing urls, line breaks, numbers, and special characters\n",
    "pdf['clean_body'] = [re.sub(r'http\\S+', '', str(x)) for x in pdf['sub_body'].values]\n",
    "pdf['clean_body'] = pdf['clean_body'].str.replace(\"\\r\", \" \")\n",
    "pdf['clean_body'] = pdf['clean_body'].str.replace(\"\\n\", \" \")\n",
    "pdf['clean_body'] = pdf['clean_body'].str.replace(\"[^a-zA-Z]\", \" \", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform NLP basic operations to tokenize, remove stop words, lemmatize and stem words\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "pdf['token_body'] = [x.lower() for x in pdf['clean_body']]\n",
    "pdf['token_body'] = pdf['token_body'].apply(word_tokenize)\n",
    "pdf['token_body'] = pdf['token_body'].apply(lambda tokens: [t for t in tokens if t not in stop_words])\n",
    "pdf['token_body'] = pdf['token_body'].apply(lambda tokens: [lemmatizer.lemmatize(t) for t in tokens])\n",
    "pdf['token_body'] = pdf['token_body'].apply(lambda tokens: [stemmer.stem(t) for t in tokens])\n",
    "pdf['final_body'] = [\" \".join(x) for x in pdf['token_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to difficulty in handling different formatting of html structure, manually removing html-related codes\n",
    "bad_word_list = ['zmnj', 'px', 'width', 'size', 'border', 'text', 'verdana', 'height'\n",
    "                    , 'pad', 'align', 'lucida', 'serif', 'display', 'import', 'tabl'\n",
    "                    , 'font', 'background', 'style', 'color', 'td', 'span', 'ding'\n",
    "                    , 'san', 'mso', 'ant', 'tr', 'class', 'none', 'email', 'center'\n",
    "                    , 'top', 'line', 'cell', 'div', ' p ', 'adjust', 'block', 'left'\n",
    "                    , 'max', 'margin', 'alt', 'unicod', 'img', 'collaps', 'bottom'\n",
    "                    , 'cont', 'famili', 'select', 'blank', 'pt', 'f', 'view', 'right'\n",
    "                    , 'content', 'mcn', 'src', 'normal', 'spac', 'ent', 'zwnj'\n",
    "                    , ' e ', ' v ', ' er ', 'min', 'webkit', 'ain', 'repeat'\n",
    "                    , 'tbodi', ' m ', 'image', 'imag', 'ree', ' ca ', ' sc '\n",
    "                    , ' o ', ' pre ', ' l ', ' u ', ' h ', ' c ', ' n ']\n",
    "\n",
    "# Function to iterate over replacing html words\n",
    "def remove_words(pdf, column, bad_word_list):\n",
    "    for word in bad_word_list:\n",
    "        pdf[column] = pdf[column].str.replace(word, \" \")\n",
    "\n",
    "    return pdf\n",
    "\n",
    "# Run the function\n",
    "pdf = remove_words(pdf, 'final_body', bad_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test datasets\n",
    "\n",
    "# Randomize the data\n",
    "pdf = pdf.sample(frac=1, random_state = random_state)\n",
    "\n",
    "# Split the datasets\n",
    "total_vec_cnt = pdf.shape[0]\n",
    "train_split = round(total_vec_cnt * train_test_split_perc)\n",
    "test_split = round(total_vec_cnt * (1 - train_test_split_perc))\n",
    "\n",
    "train_pdf = pdf.head(train_split)\n",
    "test_pdf = pdf.tail(test_split)\n",
    "\n",
    "train_corpus = train_pdf['final_body'].tolist()\n",
    "test_corpus = test_pdf['final_body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data for downstream processing\n",
    "pickle.dump(train_pdf, open(train_pdf_path, \"wb\"))\n",
    "pickle.dump(test_pdf, open(test_pdf_path, \"wb\"))\n",
    "pickle.dump(train_corpus, open(train_corpus_path, \"wb\"))\n",
    "pickle.dump(test_corpus, open(test_corpus_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1795: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. <class 'numpy.int64'> 'dtype' will be converted to np.float64.\n",
      "  warnings.warn(\"Only {} 'dtype' should be used. {} 'dtype' will \"\n"
     ]
    }
   ],
   "source": [
    "# Fit the corpus text data to the NLP models\n",
    "\n",
    "# Transform corpus to document term frequency matrix\n",
    "# We can get frequency counts for each word per document (email)\n",
    "tf_vectorizer_model = CountVectorizer()\n",
    "train_dtm_tf = tf_vectorizer_model.fit_transform(train_corpus)\n",
    "\n",
    "# Transform corpus to term frequency inverse document frequency matrix\n",
    "# We use tfidf so common words across all documents have reduced importance\n",
    "tfidf_vectorizer_model = TfidfVectorizer(**tf_vectorizer_model.get_params())\n",
    "train_dtm_tfidf = tfidf_vectorizer_model.fit_transform(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save NLP text models for later inference\n",
    "pickle.dump(train_dtm_tfidf, open(train_dtm_tfidf_path, \"wb\"))\n",
    "pickle.dump(tf_vectorizer_model, open(tf_vectorizer_model_path, \"wb\"))\n",
    "pickle.dump(tfidf_vectorizer_model, open(tfidf_vectorizer_model_path, \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
